{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset  Overview\n",
    "- Importing, Analyzing & Understanding the Data/Importing the input data files(s)\n",
    "- Initial data frame overview\n",
    "- Null-value calculation and review\n",
    "- Duplicate Analysis and review\n",
    "\n",
    "# Data Cleaning\n",
    "- Delete high-null features\n",
    "- Categorical & Numerical segregation\n",
    "- Categorical/Numerical features null-value treatment\n",
    "\n",
    "# Exploratory data analysis (EDA)\n",
    "- Data imbalance analysis\n",
    "- Categorical/Numerical features Univariate analysis\n",
    "- Categorical/Numerical features Bivariate analysis\n",
    "- Output class disbalance analisys (for classification scenarios)\n",
    "\n",
    "# Data Preparation Techniques\n",
    "- Outliers analysis and treatment\n",
    "- Binary categories treatment\n",
    "- Dummy variables substitution\n",
    "- Features scaling/normalization\n",
    "- Feature construction\n",
    "    - composing\n",
    "    - decomposing\n",
    "- Binning\n",
    "- Log transform\n",
    "- Grouping operations (based on the granularity level)\n",
    "- Train-Test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot Style\n",
    "sns.set_context(\"paper\")\n",
    "style.use('fivethirtyeight')\n",
    "\n",
    "#Sci-kit learn libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Performance Evaluation/Metrics\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#statmodel libraries\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 80)\n",
    "pd.set_option('display.max_rows', 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Input file\n",
    "# Local path to the file\n",
    "data = pd.read_csv(\"fileName.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of columns\n",
    "len(data.columns)\n",
    "\n",
    "# Data shape\n",
    "print(\"Data dimension:\",lead.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional data shape analysis\n",
    "data.info()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persisting processed data\n",
    "data.to_csv(\"fileName.csv\")\n",
    "data_processed = pd.read_csv(\"fileName_Processed.csv\")\n",
    "data_processed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data_processed['targetColumn']\n",
    "X = data_processed.drop(['targetColumn'], axis=1)\n",
    "\n",
    "# Splitting the data into train (70%) and test (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.7, test_size=0.3, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Redundant Feature Elimination (RFE) to reduce the feature count from 49 to 20\n",
    "logreg = LogisticRegression()\n",
    "rfe = RFE(logreg, 20)           \n",
    "rfe = rfe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking which columns remained after RFE\n",
    "rfe_col = X_train.columns[rfe.support_]\n",
    "rfe_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking which columns were eliminated after RFE\n",
    "X_train.columns[~rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_logistic_model(feature_list):\n",
    "    X_train_local = X_train[feature_list]\n",
    "    log_model =  LogisticRegression(solver='liblinear').fit(X_train_local, y_train)\n",
    "    return(log_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the features list\n",
    "features = list(rfe_col)\n",
    "\n",
    "# Building the regression model\n",
    "log_model = build_logistic_model(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizing the created model to make predictions using the 'predict_proba' functionality for the train set\n",
    "# 'predict_proba' generates the probabilities for the target in array form\n",
    "y_train_pred = log_model.predict_proba(X_train[features])\n",
    "y_train_pred\n",
    "y_train_pred[:,[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizing the created model to make predictions using the 'predict_proba' functionality for the test set\n",
    "y_test_pred = log_model3.predict_proba(X_test[features])\n",
    "y_test_pred[:,[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model`s performance metrics & evaluation\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test,y_test_pred)\n",
    "print(classification_report(y_test,y_test_pred))\n",
    "'\\n'\n",
    "print(conf_matrix)\n",
    "\n",
    "tn = conf_matrix[0,0]\n",
    "fp = conf_matrix[0,1]\n",
    "tp = conf_matrix[1,1]\n",
    "fn = conf_matrix[1,0]\n",
    "\n",
    "total = tn + fp + tp + fn\n",
    "accuracy  = (tp + tn) / total # Accuracy Rate\n",
    "precision = tp / (tp + fp) # Positive Predictive Value\n",
    "recall    = tp / (tp + fn) # True Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset features\n",
    "X_test[features].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persisting the created regression model using Pickle\n",
    "import pickle\n",
    "\n",
    "with open(\"leadScoringModelName.pkl\", 'wb') as f:\n",
    "        pickle.dump(log_model, f)\n",
    "print(\"Model has been pickled. Run /score to score model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and executing the saved regressor using Pickle \n",
    "lead_scoring_model = pickle.load(open(\"LeadScoringModelName.pkl\", 'rb'))\n",
    "result = lead_scoring_model.score(X_test[features], y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing ONNX related packages for converting the Scikit-learn`s model into the OONX model`s format\n",
    "\n",
    "# Importing ONNX related packages\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "\n",
    "# Importing ONNX Runtime related package\n",
    "import onnxruntime as rt\n",
    "\n",
    "# skl2onnx.get_latest_tested_opset_version()\n",
    "# skl2onnx.supported_converters(from_sklearn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local path for persisting the ONNX model`s format\n",
    "ONNXModelPath = \"leadScoringModel.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the input features dimension as FloatTensorType\n",
    "num_features = 10\n",
    "initial_type = [('feature_input', FloatTensorType([None, num_features]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the Scikit-learn`s regression model into the ONNX format\n",
    "onnx = convert_sklearn(lead_scoring_model, initial_types=initial_type)\n",
    "\n",
    "# Saving the model on the previously defined local path\n",
    "with open(ONNXModelPath, \"wb\") as f:\n",
    "    f.write(onnx.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating inference session for runtime interaction\n",
    "session = rt.InferenceSession(ONNXModelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.get_inputs()[0].name)\n",
    "input_name = session.get_inputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.get_outputs()[0].name)\n",
    "label_name = session.get_outputs()[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing the session and retrieving the results\n",
    "pred_onnx = session.run(None, {input_name: X_train[features].values.astype(np.float32)})[1]\n",
    "pred_onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[features].values[0]\n",
    "X_train[features].values[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
